{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Stack Exchange post text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_csv(\"/Users/christinachang/Documents/STA141C/sta-141c-classify/data/biology.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(doc):\n",
    "    \"\"\"Combine the strings in the \"response\" column of dataframe df into one long string. Then, tokenize the\n",
    "    string and make all words lowercase.\"\"\"\n",
    "\n",
    "    # Tokenize and make lowercase.\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(words):\n",
    "    \"\"\"Lemmatize words to get the base words. The input 'words' is a list of of words.\"\"\"\n",
    "    \n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    word_tags = nltk.pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(w, wordnet_pos(t)) for (w, t) in word_tags]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stopwords from a string.\"\"\"\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def clean_text(doc):    \n",
    "    \"\"\"Tokenize, lemmatize, and remove stopwords for the text of all articles.\"\"\"\n",
    "    \n",
    "    words = re.sub(\"< ?/?[a-z]+ ?>|\\n\", \"\", doc)\n",
    "    words = tokenize_text(words)\n",
    "    words = lemmatize_text(words)\n",
    "    words = remove_stopwords(words)\n",
    "    doc = [w for w in words if w.isalnum()]\n",
    "    doc = ' '.join(doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_df(df):\n",
    "    \"\"\"Combine the title and content of each post into one string and clean each string.\"\"\"\n",
    "    text = df['title'] + \" \" + df['content']\n",
    "    df_clean = pd.DataFrame([clean_text(i) for i in text])\n",
    "    df_clean.columns = [\"text\"]\n",
    "    df_clean[\"tags\"] = df[\"tags\"]\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack exchange topic names\n",
    "names = [\"biology\",\"cooking\",\"crypto\",\"diy\",\"robotics\",\"travel\"]\n",
    "\n",
    "def get_paths(name):\n",
    "    \"\"\"Get path names for each file.\"\"\"\n",
    "    path = \"data/\"+name+\".csv\"\n",
    "    return path\n",
    "\n",
    "# Get path names\n",
    "paths = [get_paths(i) for i in names]\n",
    "\n",
    "# All data frames in a list.\n",
    "dfs = [pd.read_csv(i) for i in paths]\n",
    "\n",
    "# Get a list of the cleaned data frames.\n",
    "clean_dfs = [clean_df(i) for i in dfs]\n",
    "\n",
    "# Save cleaned dfs as csv\n",
    "for i in range(len(names)):\n",
    "    clean_dfs[i].to_pickle(names[i]+\"_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with subset of data.\n",
    "subsets = [df[0:50] for df in dfs]\n",
    "tmp1 = [clean_df(i) for i in subsets]\n",
    "\n",
    "# See the file\n",
    "pd.read_pickle(\"cleaned/biology_clean.dat\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
