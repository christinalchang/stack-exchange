{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(doc):\n",
    "    \"\"\"Combine the strings in the \"response\" column of dataframe df into one long string. Then, tokenize the\n",
    "    string and make all words lowercase.\"\"\"\n",
    "\n",
    "    # Tokenize and make lowercase.\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    words = [w.lower() for w in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "    \n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "    \n",
    "    # Default to a noun.\n",
    "    return table.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize_text(words):\n",
    "    \"\"\"Lemmatize words to get the base words. The input 'words' is a list of of words.\"\"\"\n",
    "    \n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    word_tags = nltk.pos_tag(words)\n",
    "    words = [lemmatizer.lemmatize(w, wordnet_pos(t)) for (w, t) in word_tags]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stopwords from a string.\"\"\"\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def clean_text(doc):    \n",
    "    \"\"\"Tokenize, lemmatize, and remove stopwords for the text of all articles.\"\"\"\n",
    "    \n",
    "    words = re.sub(\"< ?/?[a-z]+ ?>|\\n\", \"\", doc)\n",
    "    words = tokenize_text(words)\n",
    "    words = lemmatize_text(words)\n",
    "    words = remove_stopwords(words)\n",
    "    doc = [w for w in words if w.isalnum()]\n",
    "    doc = ' '.join(doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def clean_df(df):\n",
    "    \"\"\"Combine the title and content of each post into one string and clean each string.\"\"\"\n",
    "    text = df['title'] + \" \" + df['content']\n",
    "    df_clean = pd.DataFrame([clean_text(i) for i in text])\n",
    "    df_clean.columns = [\"text\"]\n",
    "    #df_clean[\"tags\"] = df[\"tags\"]\n",
    "    df_clean = pd.concat([df_clean, pd.DataFrame(df[\"tags\"])],axis = 1, sort = False)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions from examine_tags.ipynb\n",
    "def get_top_tags(df):\n",
    "    tag_list = [tags.split(' ') for tags in df['tags']]\n",
    "    flat_list = [item for sublist in tag_list for item in sublist]\n",
    "    fq = nltk.FreqDist(w for w in flat_list)\n",
    "    df_fq = pd.DataFrame.from_dict(fq, orient=\"index\").reset_index()\n",
    "    df_95 = df_fq[df_fq.iloc[:,1] >= np.percentile(df_fq.iloc[:,1],95)].reset_index(drop=True)\n",
    "    df_95.columns = [\"term\", \"fq\"]\n",
    "    return df_95\n",
    "\n",
    "def subset_top_df(df, top_tags_df):\n",
    "    \"\"\"\n",
    "    df: DataFrame with all posts\n",
    "    top_tags_df: Data frame of top tags\n",
    "    \"\"\"\n",
    "    tags_list = [tags.split(' ') for tags in df['tags']]\n",
    "    indeces = [i for i in range(len(tags_list))\n",
    "               if list(set(top_tags_df['term']) & \n",
    "                       set(tags_list[i])) != []]\n",
    "    return df.loc[indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tags_clean(df):\n",
    "    top = get_top_tags(df)\n",
    "    top_subset = subset_top_df(df, top).reset_index()\n",
    "    return clean_df(top_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack exchange topic names\n",
    "names = [\"biology\",\"cooking\",\"crypto\",\"diy\",\"robotics\",\"travel\"]\n",
    "\n",
    "def get_paths(name):\n",
    "    \"\"\"Get path names for each file.\"\"\"\n",
    "    path = \"data/\"+name+\".csv\"\n",
    "    return path\n",
    "\n",
    "# Get path names\n",
    "paths = [get_paths(i) for i in names]\n",
    "\n",
    "# All data frames in a list.\n",
    "dfs = [pd.read_csv(i) for i in paths]\n",
    "\n",
    "# Get a list of the cleaned data frames.\n",
    "trim_clean_dfs = [trim_tags_clean(i) for i in dfs]\n",
    "\n",
    "# Save cleaned dfs as csv\n",
    "for i in range(len(names)):\n",
    "    trim_clean_dfs[i].to_pickle(names[i]+\"_trim_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
